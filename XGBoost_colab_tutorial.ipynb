{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15ddacb9",
      "metadata": {
        "id": "15ddacb9"
      },
      "source": [
        "# XGBoost for Heart Disease Prediction: A Comprehensive Tutorial\n",
        "\n",
        "This explainable AI (XAI) tutorial was developed by [Dr. Stephanie Kelley](https://www.stephaniekelleyresearch.com/), Scotiabank Professor in Innovations in Business Technology at the Sobey School of Business at Saint Mary's University with the assistance of her graduate research assistant Muhammad Uzair Tahir.\n",
        "\n",
        "Published: May 2025, Last Updated: May 2025\n",
        "\n",
        "# 1. Introduction\n",
        "- Heart disease remains one of the leading causes of mortality worldwide, accounting for approximately 32% of all deaths globally. Early and accurate prediction of heart disease can significantly improve patient outcomes through timely intervention and treatment.\n",
        "- Machine learning algorithms have shown remarkable potential in medical diagnostics by identifying patterns in patient data that might not be immediately apparent to human clinicians. Among these algorithms, ensemble methods like XGBoost have demonstrated exceptional performance in various healthcare applications.\n",
        "\n",
        "### What is XGBoost?\n",
        "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithms designed for speed and performance. It works by building an ensemble of decision trees sequentially, where each new tree corrects the errors made by the previous trees.\n",
        "\n",
        "**Key advantages of XGBoost include:**\n",
        "\n",
        "- High performance: Optimized for both speed and prediction accuracy\n",
        "- Regularization: Built-in capabilities to prevent overfitting\n",
        "- Handling missing values: Can work with incomplete data\n",
        "- Flexibility: Works well with various types of data and problems\n",
        "\n",
        "In this tutorial, we will explore how to use XGBoost to predict heart disease based on patient characteristics and medical measurements. We'll walk through each step of the machine learning workflow, from data exploration to model interpretation.\n",
        "\n",
        "The data for the tutorial from Manu Siddhartha. (2020). Heart Disease Dataset (Comprehensive). IEEE Dataport. https://dx.doi.org/10.21227/dz4t-cm36.\n",
        "A copy of the data is already accessible within the tutorial, however the original source is https://ieee-dataport.org/open-access/heart-disease-dataset-comprehensive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6260ca15",
      "metadata": {
        "id": "6260ca15"
      },
      "source": [
        "## 2. Setting Up the Environment\n",
        "Before we begin, let's import the necessary libraries and set up our environment. Each library serves a specific purpose in our analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49737a7d",
      "metadata": {
        "id": "49737a7d"
      },
      "outputs": [],
      "source": [
        "!pip install lime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "from lime import lime_tabular\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "plt.style.use('ggplot')\n",
        "sns.set(font_scale=1.2)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdba397",
      "metadata": {
        "id": "8bdba397"
      },
      "source": [
        "**Libraries Description**\n",
        "\n",
        "- **pandas and numpy:** Essential for data manipulation and numerical computations\n",
        "\n",
        "- **matplotlib and seaborn:** Create visualizations to understand the data\n",
        "\n",
        "- **scikit-learn:** Provides tools for model evaluation and data preprocessing\n",
        "\n",
        "- **xgboost:** Implements the XGBoost algorithm\n",
        "\n",
        "- **shap and lime:** Provide tools for explaining model predictions\n",
        "\n",
        "- **warnings and os:** Utility modules for better notebook management"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe5f361",
      "metadata": {
        "id": "3fe5f361"
      },
      "source": [
        "# 3. Data Loading and Understanding\n",
        "Now we'll load our heart disease dataset and explore its structure. Understanding the data is a crucial first step in any machine learning project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c657ab",
      "metadata": {
        "id": "77c657ab"
      },
      "outputs": [],
      "source": [
        "def load_data(file_url=\"https://raw.githubusercontent.com/stephaniekelley/ExplainableAITutorial/main/heartdiseasedataset.csv\"):\n",
        "    print(f\"Loading data from {file_url}...\")\n",
        "\n",
        "    # Read CSV from GitHub\n",
        "    df = pd.read_csv(file_url)\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    column_names = {\n",
        "        'age': 'Age',\n",
        "        'sex': 'Sex',\n",
        "        'chest pain type': 'ChestPainType',\n",
        "        'resting bp s': 'RestingBP',\n",
        "        'cholesterol': 'Cholesterol',\n",
        "        'fasting blood sugar': 'FastingBS',\n",
        "        'resting ecg': 'RestingECG',\n",
        "        'max heart rate': 'MaxHR',\n",
        "        'exercise angina': 'ExerciseAngina',\n",
        "        'oldpeak': 'Oldpeak',\n",
        "        'ST slope': 'ST_Slope',\n",
        "        'target': 'Target'\n",
        "    }\n",
        "    df = df.rename(columns=column_names)\n",
        "\n",
        "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "\n",
        "    print(\"\\nFirst few rows of the dataset:\")\n",
        "    display(df.head())\n",
        "\n",
        "    print(\"\\nDataset information:\")\n",
        "    display(df.info())\n",
        "\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    display(df.describe())\n",
        "\n",
        "    print(\"\\nTarget Distribution:\")\n",
        "    display(df['Target'].value_counts())\n",
        "\n",
        "    target_counts = df['Target'].value_counts(normalize=True) * 100\n",
        "    print(f\"\\nPercentage of patients with heart disease: {target_counts[1]:.2f}%\")\n",
        "    print(f\"Percentage of patients without heart disease: {target_counts[0]:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Call the function\n",
        "df = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2b4a691",
      "metadata": {
        "id": "a2b4a691"
      },
      "source": [
        "**Display detailed information about categorical variables**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a74e781",
      "metadata": {
        "id": "5a74e781"
      },
      "outputs": [],
      "source": [
        "categorical_descriptions = {\n",
        "    'Sex': {1: 'Male', 0: 'Female'},\n",
        "    'ChestPainType': {1: 'Typical Angina', 2: 'Atypical Angina', 3: 'Non-anginal Pain', 4: 'Asymptomatic'},\n",
        "    'FastingBS': {1: 'True (>120 mg/dl)', 0: 'False (≤120 mg/dl)'},\n",
        "    'RestingECG': {0: 'Normal', 1: 'ST-T wave abnormality', 2: 'Left ventricular hypertrophy'},\n",
        "    'ExerciseAngina': {1: 'Yes', 0: 'No'},\n",
        "    'ST_Slope': {1: 'Upsloping', 2: 'Flat', 3: 'Downsloping'},\n",
        "    'Target': {1: 'Heart Disease', 0: 'No Heart Disease'}\n",
        "}\n",
        "print(\"Categorical Variable Descriptions:\")\n",
        "for variable, descriptions in categorical_descriptions.items():\n",
        "    print(f\"\\n{variable}:\")\n",
        "    for code, description in descriptions.items():\n",
        "        count = df[df[variable] == code].shape[0]\n",
        "        percentage = (count / df.shape[0]) * 100\n",
        "        print(f\"  {code}: {description} - {count} patients ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75fa3d95",
      "metadata": {
        "id": "75fa3d95"
      },
      "source": [
        "# 4. Exploratory Data Analysis (EDA)\n",
        "Exploratory Data Analysis helps us understand patterns and relationships in the data through visualization. This knowledge guides our modeling approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013d935e",
      "metadata": {
        "id": "013d935e"
      },
      "outputs": [],
      "source": [
        "def explore_data(df):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "\n",
        "    print(\"Exploring numerical features distribution...\")\n",
        "    plt.figure(figsize=(18, 12), dpi=300)\n",
        "    numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        sns.histplot(data=df, x=col, hue='Target', multiple='stack',\n",
        "                     palette='Set1', kde=True, bins=30)\n",
        "        plt.title(f'Distribution of {col} by Heart Disease Status', fontsize=14)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        mean_disease = df[df['Target'] == 1][col].mean()\n",
        "        mean_no_disease = df[df['Target'] == 0][col].mean()\n",
        "        plt.axvline(mean_disease, color='red', linestyle='--',\n",
        "                   label=f'Mean (Disease): {mean_disease:.1f}')\n",
        "        plt.axvline(mean_no_disease, color='blue', linestyle='--',\n",
        "                   label=f'Mean (No Disease): {mean_no_disease:.1f}')\n",
        "        plt.legend(fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nExploring correlations between features...\")\n",
        "    plt.figure(figsize=(14, 12), dpi=300)\n",
        "    correlation_matrix = df.corr()\n",
        "    mask = np.triu(correlation_matrix)\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', mask=mask,\n",
        "                linewidths=0.5, annot_kws={\"size\": 10})\n",
        "    plt.title('Correlation Matrix of Heart Disease Features', fontsize=16)\n",
        "    plt.xticks(fontsize=12, rotation=45, ha='right')\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nKey correlations with Target (Heart Disease):\")\n",
        "    target_corr = correlation_matrix['Target'].drop('Target').sort_values(ascending=False)\n",
        "    for feature, corr in target_corr.items():\n",
        "        strength = \"strong\" if abs(corr) > 0.5 else \"moderate\" if abs(corr) > 0.3 else \"weak\"\n",
        "        direction = \"positive\" if corr > 0 else \"negative\"\n",
        "        print(f\"- {feature}: {corr:.3f} ({strength} {direction} correlation)\")\n",
        "\n",
        "explore_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c383861a",
      "metadata": {
        "id": "c383861a"
      },
      "source": [
        "# 5. Data Preparation\n",
        "\n",
        "Before building our model, we need to prepare the data by splitting it into training and testing sets and scaling the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5316a1f",
      "metadata": {
        "id": "c5316a1f"
      },
      "outputs": [],
      "source": [
        "def prepare_data(df):\n",
        "    print(\"Preparing data for modeling...\")\n",
        "    X = df.drop('Target', axis=1)\n",
        "    y = df['Target']\n",
        "    print(f\"Feature set shape: {X.shape}\")\n",
        "    print(f\"Target variable shape: {y.shape}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "    print(\"\\nClass distribution in training set:\")\n",
        "    print(pd.Series(y_train).value_counts(normalize=True) * 100)\n",
        "    print(\"\\nClass distribution in testing set:\")\n",
        "    print(pd.Series(y_test).value_counts(normalize=True) * 100)\n",
        "    print(\"\\nPerforming feature scaling on numerical features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_orig = X_train.copy()\n",
        "    X_test_orig = X_test.copy()\n",
        "    numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "    X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "    print(\"\\nComparing original vs. scaled features (first 3 rows):\")\n",
        "    comparison = pd.DataFrame({\n",
        "        'Original_Age': X_train_orig['Age'].head(3),\n",
        "        'Scaled_Age': X_train_scaled['Age'].head(3),\n",
        "        'Original_RestingBP': X_train_orig['RestingBP'].head(3),\n",
        "        'Scaled_RestingBP': X_train_scaled['RestingBP'].head(3)\n",
        "    })\n",
        "    display(comparison)\n",
        "    print(\"\\nWhy keep both scaled and unscaled versions?\")\n",
        "    print(\"- XGBoost doesn't strictly require scaling, but it can help with convergence\")\n",
        "    print(\"- Keeping unscaled data helps with interpretability of results\")\n",
        "    print(\"- We'll compare model performance with and without scaling\")\n",
        "\n",
        "    return X_train_orig, X_test_orig, y_train, y_test, X_train_scaled, X_test_scaled, X.columns\n",
        "\n",
        "X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, feature_names = prepare_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9feb67ad",
      "metadata": {
        "id": "9feb67ad"
      },
      "source": [
        "# 6. Building the XGBoost Model\n",
        "\n",
        "Now we'll build and train our XGBoost model for heart disease prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dea5ad6",
      "metadata": {
        "id": "0dea5ad6"
      },
      "outputs": [],
      "source": [
        "def train_xgboost(X_train, X_test, y_train, y_test, feature_names):\n",
        "\n",
        "    print(\"Training XGBoost model...\")\n",
        "    print(\"\\nKey XGBoost Parameters Explained:\")\n",
        "    print(\"- n_estimators: Number of boosting rounds (trees to build)\")\n",
        "    print(\"- learning_rate: Step size shrinkage to prevent overfitting\")\n",
        "    print(\"- max_depth: Maximum depth of each tree (controls complexity)\")\n",
        "    print(\"- min_child_weight: Minimum sum of instance weight in a child (controls overfitting)\")\n",
        "    print(\"- gamma: Minimum loss reduction for further partition (higher = more conservative)\")\n",
        "    print(\"- subsample: Fraction of samples used for tree building (helps prevent overfitting)\")\n",
        "    print(\"- colsample_bytree: Fraction of features used per tree (introduces randomness)\")\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        min_child_weight=1,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective='binary:logistic',\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    print(\"\\nPerforming 5-fold cross-validation...\")\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "    print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "    print(f\"Individual fold accuracies: {cv_scores}\")\n",
        "    print(\"- Average accuracy across different data subsets\")\n",
        "    print(\"- How stable/consistent our model performance is\")\n",
        "    print(\"- Whether we might be overfitting to a particular subset of data\")\n",
        "    plt.figure(figsize=(10, 8), dpi=100)\n",
        "    plt.bar(range(1, 6), cv_scores, color='skyblue')\n",
        "    plt.axhline(y=cv_scores.mean(), color='red', linestyle='--',\n",
        "                label=f'Mean Accuracy: {cv_scores.mean():.4f}')\n",
        "    plt.xlabel('Fold', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.title('5-Fold Cross-Validation Results', fontsize=14)\n",
        "    plt.xticks(range(1, 6))\n",
        "    plt.ylim(0.5, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "    print(\"\\nTraining final model on the entire training set...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"\\nMaking predictions on the test set...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "    print(\"\\nUnderstanding Classification Metrics:\")\n",
        "    print(\"- Precision: Of patients predicted to have heart disease, how many actually have it\")\n",
        "    print(\"- Recall: Of patients who actually have heart disease, how many did we identify\")\n",
        "    print(\"- F1-score: Harmonic mean of precision and recall\")\n",
        "    print(\"- Support: Number of instances in each class\")\n",
        "    plt.figure(figsize=(10, 8), dpi=100)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title('Confusion Matrix', fontsize=16)\n",
        "    plt.xlabel('Predicted Label', fontsize=14)\n",
        "    plt.ylabel('True Label', fontsize=14)\n",
        "    labels = ['No Heart Disease', 'Heart Disease']\n",
        "    plt.xticks([0.5, 1.5], labels, fontsize=12)\n",
        "    plt.yticks([0.5, 1.5], labels, fontsize=12, rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.figure(figsize=(10, 8), dpi=100)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate (1-Specificity)', fontsize=14)\n",
        "    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=14)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
        "    plt.legend(loc=\"lower right\", fontsize=12)\n",
        "    plt.grid(linestyle='--', alpha=0.3)\n",
        "    plt.show()\n",
        "    print(\"\\nUnderstanding the ROC Curve:\")\n",
        "    print(\"- X-axis: False Positive Rate (1-Specificity)\")\n",
        "    print(\"- Y-axis: True Positive Rate (Sensitivity)\")\n",
        "    print(\"- AUC (Area Under Curve): Probability that model ranks a random positive example\")\n",
        "    print(\"  higher than a random negative example\")\n",
        "    print(f\"- Our model's AUC: {roc_auc:.3f} (closer to 1 is better, 0.5 is random guessing)\")\n",
        "    plt.figure(figsize=(12, 8), dpi= 100)\n",
        "    importance = model.get_booster().get_score(importance_type='gain')\n",
        "    importance_series = pd.Series(importance)\n",
        "    missing_features = [feat for feat in feature_names if feat not in importance_series.index]\n",
        "    if missing_features:\n",
        "        print(f\"\\nNote: These features were not used by the model: {missing_features}\")\n",
        "        for feat in missing_features:\n",
        "            importance_series[feat] = 0\n",
        "    importance_series = importance_series.sort_values(ascending=True)\n",
        "    importance_series = importance_series.round(2)\n",
        "    ax = importance_series.plot(kind='barh', figsize=(12, 8))\n",
        "    for i, v in enumerate(importance_series):\n",
        "        ax.text(v + 0.1, i, f\"{v:.2f}\", va='center')\n",
        "    plt.title('XGBoost Feature Importance (Gain)', fontsize=16)\n",
        "    plt.xlabel('Importance Value (Gain)', fontsize=14)\n",
        "    plt.ylabel('Feature', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nUnderstanding Feature Importance (Gain):\")\n",
        "    print(\"- Gain represents the improvement in accuracy brought by a feature\")\n",
        "    print(\"- Higher values indicate more important features for prediction\")\n",
        "    print(\"- These values can help doctors understand key risk factors\")\n",
        "    top_features = importance_series.sort_values(ascending=False).head(3)\n",
        "    print(f\"\\nTop 3 most important features:\")\n",
        "    for feature, importance in top_features.items():\n",
        "        print(f\"- {feature}: {importance:.2f}\")\n",
        "    return model, X_test, y_test\n",
        "\n",
        "model, X_test, y_test = train_xgboost(X_train_scaled, X_test_scaled, y_train, y_test, feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbcf533",
      "metadata": {
        "id": "ccbcf533"
      },
      "source": [
        "**How XGBoost Works for Heart Disease Prediction**\n",
        "\n",
        "XGBoost builds an ensemble of decision trees sequentially. Each tree focuses on correcting the mistakes of previous trees. For heart disease prediction:\n",
        "\n",
        "**First Tree:** Makes initial predictions based on strongest indicators (like chest pain type or exercise angina)\n",
        "\n",
        "**Second Tree:** Focuses on correcting errors from the first tree, perhaps emphasizing age and resting ECG for patients misclassified initially\n",
        "\n",
        "**Subsequent Trees:** Continue refining predictions, gradually improving accuracy\n",
        "\n",
        "**Final Prediction:** Combines all trees' outputs, with each tree getting a weight based on its accuracy\n",
        "\n",
        "The final model typically includes dozens to hundreds of trees working together to make the most accurate prediction possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf87010",
      "metadata": {
        "id": "ecf87010"
      },
      "source": [
        "# 7. Model Interpretation with SHAP\n",
        "\n",
        "While accuracy is important, understanding why the model makes specific predictions is crucial in healthcare. SHAP (SHapley Additive exPlanations) values are one methods of explainable AI (XAI) that can help us understand how each feature contributes to individual predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c058c69",
      "metadata": {
        "id": "0c058c69"
      },
      "outputs": [],
      "source": [
        "def perform_shap_analysis(model, X_test, X_train, y_test, feature_names,\n",
        "                                 fig_width=12, fig_height=10, dpi=300,\n",
        "                                 max_display=10, display_size=20):\n",
        "\n",
        "    print(\"Performing enhanced SHAP (SHapley Additive exPlanations) analysis...\")\n",
        "    print(\"\\nWhat are SHAP values?\")\n",
        "    print(\"- SHAP (SHapley Additive exPlanations) values come from game theory\")\n",
        "    print(\"- They measure how much each feature contributes to a prediction\")\n",
        "    print(\"- Positive values push prediction higher, negative values push it lower\")\n",
        "    print(\"- They account for feature interactions and are consistent/fair\")\n",
        "    print(\"- SHAP provides both global (model-wide) and local (individual) explanations\")\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    print(\"\\nCalculating SHAP values for test set...\")\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    label0_idx = y_test[y_test == 0].index[0]\n",
        "    label1_idx = y_test[y_test == 1].index[0]\n",
        "\n",
        "    selected_indices = [label0_idx, label1_idx]\n",
        "    print(f\"\\nSelected patient samples for analysis:\")\n",
        "    print(f\"- Patient {label0_idx}: No heart disease\")\n",
        "    print(f\"- Patient {label1_idx}: Has heart disease\")\n",
        "    print(\"\\nCreating SHAP summary plot (global feature importance)...\")\n",
        "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names,\n",
        "                     max_display=max_display, show=False)\n",
        "    plt.title('SHAP Summary Plot: Impact of Features on Heart Disease Prediction', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nHow to interpret the SHAP summary plot:\")\n",
        "    print(\"- Features are ranked by importance from top to bottom\")\n",
        "    print(\"- Each point represents one patient's SHAP value for that feature\")\n",
        "    print(\"- Red points indicate higher feature values, blue points indicate lower values\")\n",
        "    print(\"- Points to the right indicate pushing prediction toward a prediction of heart disease\")\n",
        "    print(\"- Points to the left indicate pushing prediction away from a prediction of heart disease\")\n",
        "    print(\"\\nCreating SHAP bar plot (average feature impact)...\")\n",
        "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names,\n",
        "                     max_display=max_display, plot_type='bar', show=False)\n",
        "    plt.title('SHAP Feature Importance: Average Impact on Model Output Magnitude', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nCreating SHAP beeswarm plot...\")\n",
        "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "    shap.plots.beeswarm(shap.Explanation(values=shap_values,\n",
        "                                      data=X_test,\n",
        "                                      feature_names=list(feature_names)),\n",
        "                     max_display=max_display, show=False)\n",
        "    plt.title('SHAP Beeswarm Plot: Detailed Feature Impact Distribution', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nCreating SHAP violin plot...\")\n",
        "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names,\n",
        "                    max_display=max_display, plot_type=\"violin\", show=False)\n",
        "    plt.title('SHAP Violin Summary Plot: Feature Impact Distribution', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    top_features = np.argsort(np.abs(shap_values).mean(0))[-3:]\n",
        "    top_feature_names = [feature_names[i] for i in top_features]\n",
        "    print(f\"\\nTop 3 most important features: {', '.join(top_feature_names)}\")\n",
        "    print(\"\\nCreating SHAP dependence plots for top features...\")\n",
        "    for feature_idx in top_features:\n",
        "        feature_name = feature_names[feature_idx]\n",
        "        plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "        shap.dependence_plot(\n",
        "            feature_idx,\n",
        "            shap_values,\n",
        "            X_test,\n",
        "            feature_names=feature_names,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Dependence Plot: How {feature_name} Impacts Predictions', fontsize=14)\n",
        "        plt.xlabel(f'{feature_name} Value', fontsize=12)\n",
        "        plt.ylabel(f'SHAP Value (impact on prediction)', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nInsights about {feature_name}:\")\n",
        "        if feature_name == 'ChestPainType':\n",
        "            print(\"- Higher chest pain types (especially type 4: Asymptomatic) strongly push predictions toward heart disease\")\n",
        "            print(\"- This suggests that asymptomatic chest pain, paradoxically, is a strong indicator of underlying disease\")\n",
        "        elif feature_name == 'Age':\n",
        "            print(\"- Advanced age generally increases prediction of heart disease\")\n",
        "            print(\"- The relationship appears somewhat non-linear with steeper increases at certain age thresholds\")\n",
        "        elif feature_name == 'ExerciseAngina':\n",
        "            print(\"- Presence of exercise-induced angina (value 1) strongly pushes predictions toward heart disease\")\n",
        "            print(\"- This aligns with medical knowledge that pain during exertion is a classic heart disease symptom\")\n",
        "        elif feature_name == 'Oldpeak':\n",
        "            print(\"- Higher ST depression values consistently push predictions toward heart disease\")\n",
        "            print(\"- This confirms the diagnostic value of this ECG measurement\")\n",
        "        else:\n",
        "            average_impact = np.abs(shap_values[:, feature_idx]).mean()\n",
        "            direction = \"positive\" if shap_values[:, feature_idx].mean() > 0 else \"negative\"\n",
        "            print(f\"- Has an average absolute impact of {average_impact:.3f} on predictions\")\n",
        "            print(f\"- Generally shows a {direction} relationship with heart disease prediction\")\n",
        "\n",
        "    print(\"\\nCreating SHAP heatmap for a subset of samples...\")\n",
        "    try:\n",
        "        if len(X_test) > 1:\n",
        "            plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "            shap_values_for_heatmap = shap_values[:min(display_size, len(X_test))]\n",
        "            X_test_for_heatmap = X_test.iloc[:min(display_size, len(X_test))]\n",
        "            shap.plots.heatmap(shap.Explanation(values=shap_values_for_heatmap,\n",
        "                                           data=X_test_for_heatmap,\n",
        "                                           feature_names=list(feature_names)),\n",
        "                           max_display=max_display, show=False)\n",
        "            plt.title('SHAP Heatmap: Feature Impact Across Multiple Patients', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print(\"\\nHow to interpret the SHAP heatmap:\")\n",
        "            print(\"- Each row represents a patient\")\n",
        "            print(\"- Each column represents a feature\")\n",
        "            print(\"- Red cells indicate positive SHAP values (increasing prediction)\")\n",
        "            print(\"- Blue cells indicate negative SHAP values (decreasing prediction)\")\n",
        "            print(\"- The intensity of the color represents the magnitude of the impact\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Could not create SHAP heatmap: {e}\")\n",
        "\n",
        "    print(\"\\nCreating SHAP decision plot for multiple samples...\")\n",
        "    try:\n",
        "        plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "        sample_indices = np.random.choice(range(len(X_test)), min(display_size, len(X_test)), replace=False)\n",
        "        shap.decision_plot(explainer.expected_value,\n",
        "                         shap_values[sample_indices],\n",
        "                         X_test.iloc[sample_indices].values,\n",
        "                         feature_names=list(feature_names),\n",
        "                         feature_display_range=max_display,\n",
        "                         show=False)\n",
        "        plt.title('SHAP Decision Plot: Feature Contribution Path', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"\\nHow to interpret the SHAP decision plot:\")\n",
        "        print(\"- Each line represents a patient\")\n",
        "        print(\"- The x-axis shows the SHAP value (model output)\")\n",
        "        print(\"- Features are ordered by importance\")\n",
        "        print(\"- The path shows how each feature contributes to the final prediction\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Could not create SHAP decision plot: {e}\")\n",
        "    print(\"\\nAnalyzing individual patient cases...\")\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        test_idx = X_test.index.get_loc(idx)\n",
        "\n",
        "        print(f\"\\nPatient {idx} (True label: {'Heart Disease' if y_test.loc[idx] == 1 else 'No Heart Disease'}):\")\n",
        "        print(\"\\nKey characteristics:\")\n",
        "        for feature in feature_names:\n",
        "            value = X_test.iloc[test_idx][feature]\n",
        "            print(f\"- {feature}: {value}\")\n",
        "        patient_prob = model.predict_proba(X_test.iloc[[test_idx]])[:, 1][0]\n",
        "        print(f\"\\nModel prediction: {'Heart Disease' if patient_prob > 0.5 else 'No Heart Disease'}\")\n",
        "        print(f\"Prediction probability: {patient_prob:.4f}\")\n",
        "        plt.figure(figsize=(fig_width, fig_height//2), dpi=dpi)\n",
        "        shap.force_plot(\n",
        "            explainer.expected_value,\n",
        "            shap_values[test_idx:test_idx+1, :],\n",
        "            X_test.iloc[test_idx:test_idx+1, :],\n",
        "            feature_names=list(feature_names),\n",
        "            matplotlib=True,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Force Plot for Patient {idx}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "        exp = shap.Explanation(\n",
        "            values=shap_values[test_idx],\n",
        "            base_values=explainer.expected_value,\n",
        "            data=X_test.iloc[test_idx].values,\n",
        "            feature_names=list(feature_names)\n",
        "        )\n",
        "\n",
        "        shap.plots.waterfall(\n",
        "            exp,\n",
        "            max_display=max_display,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Waterfall Plot: Explaining Prediction for Patient {idx}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        try:\n",
        "            plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "            shap.decision_plot(\n",
        "                explainer.expected_value,\n",
        "                shap_values[test_idx:test_idx+1],\n",
        "                X_test.iloc[test_idx:test_idx+1].values,\n",
        "                feature_names=list(feature_names),\n",
        "                feature_display_range=max_display,\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Decision Plot for Patient {idx}', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Note: Could not create SHAP decision plot for patient {idx}: {e}\")\n",
        "        print(\"\\nHow to read this waterfall plot:\")\n",
        "        print(\"- The base value is the average prediction across all patients\")\n",
        "        print(\"- Red bars push the prediction toward heart disease\")\n",
        "        print(\"- Blue bars push the prediction away from heart disease\")\n",
        "        print(\"- The final prediction combines all these effects\")\n",
        "        feature_impacts = list(zip(feature_names, shap_values[test_idx]))\n",
        "        sorted_impacts = sorted(feature_impacts, key=lambda x: abs(x[1]), reverse=True)\n",
        "        print(\"\\nTop features influencing this prediction:\")\n",
        "        for feature, impact in sorted_impacts[:5]:\n",
        "            direction = \"increased\" if impact > 0 else \"decreased\"\n",
        "            print(f\"- {feature}: {impact:.4f} ({direction} likelihood of heart disease)\")\n",
        "    try:\n",
        "        print(\"\\nComputing SHAP interaction values...\")\n",
        "        sample_size = min(100, len(X_test))\n",
        "        X_sample = X_test.iloc[:sample_size]\n",
        "        interaction_values = explainer.shap_interaction_values(X_sample)\n",
        "        for feature_idx in top_features:\n",
        "            feature_name = feature_names[feature_idx]\n",
        "            plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "            shap.summary_plot(\n",
        "                interaction_values[:, feature_idx, :],\n",
        "                X_sample,\n",
        "                feature_names=list(feature_names),\n",
        "                max_display=max_display,\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Interaction: {feature_name} with Other Features', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print(f\"\\nInteraction insights for {feature_name}:\")\n",
        "            print(f\"- Shows how {feature_name} interacts with other features\")\n",
        "            print(\"- Stronger interactions indicate features that work together in predictions\")\n",
        "            print(\"- This helps identify synergistic relationships between risk factors\")\n",
        "        plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
        "        interaction_vals = np.abs(interaction_values).sum((0, 2))\n",
        "        feature_importance = pd.Series(interaction_vals, index=feature_names).sort_values(ascending=True)\n",
        "        if len(feature_importance) > max_display:\n",
        "            feature_importance = feature_importance.iloc[-max_display:]\n",
        "        feature_importance.plot.barh()\n",
        "        plt.title('Total SHAP Interaction Importance', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nNote: Could not compute SHAP interaction values: {e}\")\n",
        "        print(\"This is normal for larger datasets as interaction computations are memory-intensive.\")\n",
        "    features_shap_values = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Absolute_SHAP_Mean': np.abs(shap_values).mean(0),\n",
        "        'SHAP_Mean': shap_values.mean(0)\n",
        "    })\n",
        "    features_shap_values = features_shap_values.sort_values('Absolute_SHAP_Mean', ascending=False)\n",
        "    print(\"\\nSHAP Feature Importance Summary:\")\n",
        "    print(features_shap_values)\n",
        "    return shap_values, explainer, selected_indices\n",
        "shap_values, explainer, selected_indices = perform_shap_analysis(\n",
        "    model,\n",
        "    X_test_scaled,\n",
        "    X_train_scaled,\n",
        "    y_test,\n",
        "    feature_names,\n",
        "    fig_width=14,\n",
        "    fig_height=10,\n",
        "    dpi=120,\n",
        "    max_display=8,\n",
        "    display_size=15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b96d0a",
      "metadata": {
        "id": "a9b96d0a"
      },
      "source": [
        "# 8. Local Interpretable Model-agnostic Explanations (LIME)\n",
        "LIME provides another approach to explanation by approximating the complex model with a simpler, interpretable model around specific predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ce65f0",
      "metadata": {
        "id": "39ce65f0"
      },
      "outputs": [],
      "source": [
        "def perform_lime_analysis(model, X_train, X_test, y_test, feature_names, selected_indices):\n",
        "\n",
        "    print(\"Performing LIME (Local Interpretable Model-agnostic Explanations) analysis...\")\n",
        "    print(\"\\nWhat is LIME?\")\n",
        "    print(\"- LIME creates a simpler model that approximates the complex model's behavior locally\")\n",
        "    print(\"- It generates variations of the input and sees how predictions change\")\n",
        "    print(\"- This helps understand which features drive individual predictions\")\n",
        "    print(\"- Unlike SHAP, LIME is model-agnostic (works with any model)\")\n",
        "    print(\"- LIME and SHAP provide complementary insights\")\n",
        "    lime_explainer = lime_tabular.LimeTabularExplainer(\n",
        "        X_train.values,\n",
        "        feature_names=feature_names,\n",
        "        class_names=['No Heart Disease', 'Heart Disease'],\n",
        "        mode='classification',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    def model_predict_proba(x):\n",
        "        return model.predict_proba(x)\n",
        "\n",
        "    for idx in selected_indices:\n",
        "        test_idx = X_test.index.get_loc(idx)\n",
        "\n",
        "        print(f\"\\nLIME explanation for Patient {idx} (True label: {'Heart Disease' if y_test.loc[idx] == 1 else 'No Heart Disease'}):\")\n",
        "\n",
        "        explanation = lime_explainer.explain_instance(\n",
        "            X_test.iloc[test_idx].values,\n",
        "            model_predict_proba,\n",
        "            num_features=10\n",
        "        )\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        explanation.as_pyplot_figure()\n",
        "        plt.title(f'LIME Explanation for Patient {idx}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(f\"\\nLIME Feature Weights for Patient {idx}:\")\n",
        "        print(\"(Positive values support heart disease prediction, negative values contradict it)\")\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"{'Feature':<30} {'Weight':>10}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for feature, weight in explanation.as_list():\n",
        "            print(f\"{feature:<30} {weight:>10.4f}\")\n",
        "\n",
        "        print(\"\\nClinical Interpretation:\")\n",
        "        sorted_features = sorted(explanation.as_list(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "        top_positive = [f for f in explanation.as_list() if f[1] > 0][:3]\n",
        "        top_negative = [f for f in explanation.as_list() if f[1] < 0][:3]\n",
        "\n",
        "        if top_positive:\n",
        "            print(\"Risk factors suggesting heart disease:\")\n",
        "            for feature, weight in top_positive:\n",
        "                print(f\"- {feature} (impact: {weight:.4f})\")\n",
        "\n",
        "        if top_negative:\n",
        "            print(\"\\nProtective factors suggesting no heart disease:\")\n",
        "            for feature, weight in top_negative:\n",
        "                print(f\"- {feature} (impact: {weight:.4f})\")\n",
        "        print(\"\\nComparison with SHAP analysis:\")\n",
        "        print(\"- LIME builds a local linear model to approximate XGBoost's behavior\")\n",
        "        print(\"- SHAP calculates exact contribution values based on game theory\")\n",
        "        print(\"- Both approaches help explain complex model predictions\")\n",
        "        print(\"- Some differences in feature importance are expected due to different methodologies\")\n",
        "\n",
        "perform_lime_analysis(model, X_train_scaled, X_test_scaled, y_test, feature_names, selected_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9a67ae",
      "metadata": {
        "id": "ca9a67ae"
      },
      "source": [
        "**Why use Multiple Explanation Methods?**\n",
        "\n",
        "Using both SHAP and LIME provides robust explanation:\n",
        "\n",
        "**Methodological Triangulation:** When both methods highlight the same features, we can be more confident in their importance\n",
        "\n",
        "**Different Perspectives:** SHAP provides mathematically rigorous feature contributions, while LIME gives an intuitive local approximation\n",
        "\n",
        "**Communication Options:** Different stakeholders (doctors, patients, researchers) may find one explanation format more intuitive than another\n",
        "\n",
        "**Validation:** Comparing explanations can help identify potential model issues or data biases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30fdd2cd",
      "metadata": {
        "id": "30fdd2cd"
      },
      "source": [
        "# 9. Counterfactual Explanations with DiCE\n",
        "While SHAP and LIME help us understand why a model made a specific prediction, counterfactual explanations answer a different question: \"What would need to change about this patient to get a different prediction?\" DiCE (Diverse Counterfactual Explanations) provides actionable insights for both clinicians and patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27b5f4d",
      "metadata": {
        "id": "c27b5f4d"
      },
      "outputs": [],
      "source": [
        "!pip install dice_ml\n",
        "import dice_ml\n",
        "from dice_ml.utils import helpers\n",
        "warnings.filterwarnings('ignore')\n",
        "train_data = X_train.copy()\n",
        "train_data['Target'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['Target'] = y_test\n",
        "\n",
        "continuous_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "categorical_features = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    train_data[feature] = train_data[feature].astype(int)\n",
        "    test_data[feature] = test_data[feature].astype(int)\n",
        "\n",
        "\n",
        "print(\"Feature data types after conversion:\")\n",
        "print(train_data[categorical_features].dtypes)\n",
        "\n",
        "combined_data = pd.concat([train_data, test_data])\n",
        "d = dice_ml.Data(dataframe=combined_data,\n",
        "                 continuous_features=continuous_features,\n",
        "                 categorical_features=categorical_features,\n",
        "                 outcome_name='Target')\n",
        "class XGBoostModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_temp = X.copy()\n",
        "        for col in X_temp.columns:\n",
        "            if col in categorical_features and col in X_temp.columns:\n",
        "                X_temp[col] = X_temp[col].astype(int)\n",
        "        return self.model.predict(X_temp)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_temp = X.copy()\n",
        "        for col in X_temp.columns:\n",
        "            if col in categorical_features and col in X_temp.columns:\n",
        "                X_temp[col] = X_temp[col].astype(int)\n",
        "        return self.model.predict_proba(X_temp)\n",
        "model_wrapper = XGBoostModelWrapper(model)\n",
        "m = dice_ml.Model(model=model_wrapper, backend=\"sklearn\")\n",
        "exp = dice_ml.Dice(d, m, method=\"random\")\n",
        "for idx in selected_indices:\n",
        "    try:\n",
        "        query_instance = X_test.loc[[idx]]\n",
        "        for feature in categorical_features:\n",
        "            if feature in query_instance.columns:\n",
        "                query_instance[feature] = query_instance[feature].astype(int)\n",
        "        print(f\"\\nGenerating counterfactuals for Patient {idx}:\")\n",
        "        y_true = y_test.loc[idx]\n",
        "        y_pred = model.predict(query_instance)[0]\n",
        "        y_prob = model.predict_proba(query_instance)[0][1]\n",
        "        print(f\"True diagnosis: {'Heart Disease' if y_true == 1 else 'No Heart Disease'}\")\n",
        "        print(f\"Predicted diagnosis: {'Heart Disease' if y_pred == 1 else 'No Heart Disease'}\")\n",
        "        print(f\"Prediction probability of heart disease: {y_prob:.4f}\")\n",
        "        dice_exp = exp.generate_counterfactuals(\n",
        "            query_instance,\n",
        "            total_CFs=3,\n",
        "            desired_class=\"opposite\",\n",
        "            features_to_vary=[f for f in feature_names if f not in ['Age', 'Sex']]\n",
        "        )\n",
        "        print(\"\\nCounterfactual examples (what would need to change to get the opposite prediction):\")\n",
        "        cf_dataframe = dice_exp.visualize_as_dataframe(show_only_changes=True)\n",
        "        display(cf_dataframe)\n",
        "        if hasattr(dice_exp, 'cf_examples_list') and dice_exp.cf_examples_list:\n",
        "            cf_examples = dice_exp.cf_examples_list[0].final_cfs_df\n",
        "\n",
        "            if not cf_examples.empty:\n",
        "                original_values = query_instance.iloc[0]\n",
        "                cf_values = cf_examples.iloc[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing patient {idx}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nAnalysis complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f316315e",
      "metadata": {
        "id": "f316315e"
      },
      "source": [
        "# 9. Conclusion\n",
        "\n",
        "### Summary of XGBoost for Heart Disease Prediction\n",
        "\n",
        "In this tutorial, we've covered:\n",
        "1. Introduction\n",
        "2. Setting Up the Environment\n",
        "3. Loading and understanding heart disease data\n",
        "4. Exploratory data analysis to identify patterns\n",
        "5. Data preparation\n",
        "6. Training an XGBoost model and evaluating model performance\n",
        "7. Interpreting predictions using SHAP values\n",
        "8. Generating local explanations with LIME\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "Important limitations to consider:\n",
        "- The dataset is relatively small, which may limit generalizability\n",
        "- Some important risk factors might not be captured in our data\n",
        "- Machine learning models should supplement, not replace, clinical judgment\n",
        "- Model performs well on the test set but requires validation on external data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db79fe0",
      "metadata": {
        "id": "5db79fe0"
      },
      "source": [
        "# Additional Resources\n",
        "\n",
        "### XGBoost Documentation and Tutorials\n",
        "- [XGBoost Official Documentation](https://xgboost.readthedocs.io/)\n",
        "- [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
        "- [XGBoost Parameters Guide](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
        "\n",
        "### Explainable AI Resources\n",
        "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
        "- [LIME Github Repository](https://github.com/marcotcr/lime)\n",
        "\n",
        "### Machine Learning for Healthcare\n",
        "- [Machine Learning for Healthcare (MIT Course)](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/)\n",
        "- [AI in Medicine by Stanford](https://aim.stanford.edu/)\n",
        "- [Medical Data for Machine Learning (Kaggle)](https://www.kaggle.com/datasets?tags=13302-Medicine)\n",
        "\n",
        "### Scientific Papers\n",
        "- [Why Should I Trust You?: Explaining the Predictions of Any Classifier (LIME paper)](https://doi.org/10.48550/arXiv.1602.04938)\n",
        "- [A Unified Approach to Interpreting Model Predictions (SHAP paper)](https://doi.org/10.48550/arXiv.1705.07874)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}